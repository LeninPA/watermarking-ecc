\documentclass[a4paper,11pt]{article}

\usepackage{longtable}
\usepackage{enumitem}[shortlabels]					% Para personalizar listas
\usepackage{dirtytalk}

\usepackage[spanish, mexico]{babel}
    \decimalpoint
\usepackage[colorlinks,linkcolor=blue,urlcolor=blue,citecolor=black, breaklinks=true]{hyperref}
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{arrows}
\usetikzlibrary{patterns}
\usetikzlibrary{shapes}
\usetikzlibrary{positioning}
\usetikzlibrary{automata}
\usetikzlibrary{cd}
\usepackage{tikz-3dplot}
\usepackage{xcolor}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[intlimits]{amsmath}
\usepackage{physics}
\usepackage{fullpage}
% \usepackage[osf,sc]{mathpazo} % Uncomment for Palatino and comment out the next line
\usepackage[frenchstyle,widermath,narrowiints,fullsumlimits,fullintlimits]{kpfonts} % Comment out and uncomment the previous line for Palatino
% \linespread{1.5}
% \usepackage{helvet}
% \renewcommand{\familydefault}{\sfdefault}
\usepackage{graphicx}
\graphicspath{ {./img/} }
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amsxtra}
\usepackage{amssymb}
\usepackage{mathdots}
\usepackage{mathrsfs}
% \usepackage{microtype}
\usepackage{stmaryrd}
\usepackage{titlesec}
\usepackage{systeme}
\usepackage[titles]{tocloft}
\usepackage{textcase}
\usepackage{setspace}
    \onehalfspacing
\usepackage{xfrac}
\usepackage{mathtools}
\usepackage{faktor}
\usepackage{cancel}
\usepackage{mparhack}
\usepackage{booktabs}
\usepackage{multirow}
%\usepackage[fixlanguage]{babelbib}
\usepackage{tikz}
\usepackage{csquotes}
\usepackage{forest}
% \usepackage{siunitx}
\usepackage{xparse}
\usepackage{svg}
\usepackage{ytableau} %Diagramas de Young
\usepackage[
            hyperref=true, % También puede ser auto
            style=nature
            ]{biblatex}
\addbibresource{biblio.bib}
\usepackage{nicematrix}
\usepackage{authblk}
    \renewcommand*{\Authsep}{, }
    \renewcommand*{\Authand}{, }
    \renewcommand*{\Authands}{, }
%\usepackage{caption}
\usepackage{subcaption}
\usepackage{macros_math}
\NiceMatrixOptions{cell-space-limits = 2pt}
\usepackage{listings}[language=c]
% paquetes para obtener el 1 bomnito
\usepackage[bb=dsserif]{mathalpha}
\usepackage{bm}
\newcommand\Bbbbone{%
  \ifdefined\mathbbb%
    \mathbbb{1}%
  \else%
    \boldsymbol{\mathbb{1}}%
  \fi}

\pgfdeclarelayer{bg}
\pgfsetlayers{bg,main}

\tikzset{
  wrap/.style={
    line cap=round,
    #1,
    line width=21pt,
    opacity=0.3,
  },
  mynode/.style={
    draw,
    circle,
    yshift=-0.5cm,
    outer sep=0.3cm
  },
  group/.style={
    draw,
    ellipse,
    minimum width=3cm,
    minimum height=6cm
  },
}

\newcommand{\ttt}[1]{{\texttt{#1}}}

\usepackage[spanish]{cleveref} 

% \renewcommand{\labelenumi}{\alph{enumi})}
\title{Proyecto 2 -- Tarea-examen 2}
\author{
Lenin Pavón Alvarez
%\and 
}
%
\date{Mayo 2025}
\allowdisplaybreaks
\begin{document}
\maketitle
\begin{abstract}
    Los grandes modelos de lenguaje (LLMs por sus siglas en inglés) son capaces de generar texto (y diverso contenido multimedia) a un ritmo sin precedentes. Es así que surge la necesidad de distinguir entre aquellos productos que fueron creados por humanos y aquellos que fueron creados usando LLMs. En este proyecto planeo explorar y explicar un técnica de watermarking reciente \cite{chao_watermarking_2025} de manera computacional.
\end{abstract}
\paragraph{Palabras clave.} Teoría de Códigos\quad Procesamiento de Lenguaje Natural\quad Grandes Modelos de Lenguaje\quad Códigos Correctores de errores\quad \textit{Robust Binary Code}
\tableofcontents
\section{Antecedentes}
\subsection{Loros estocásticos (Grandes modelos de Lenguaje)}
Un trabajo que en su momento fue muy controversial (y lo sigue siendo) fue un paper por el cual una de sus autoras (Timnit Gebru) perdió su empleo en Google. En \textit{On the Dangers of Stochastic Parrots: Can language Models Be Too Big} \cite{bender_dangers_2021} las autoras advierten con respecto a la trayectoria preocupante del desarrollo de Modelos de Lenguaje y los peligros asociados a su entrenamiento y uso por la población en general.
\par Las autoras definen a un modelo de lenguaje como
\begin{displayquote}
    we understand the term \textit{language model} to refer to systems whuch are trained on string predicion tasks: that is, predicting the likelihood of a token (character, word or string) given either its preceding context or (in bidirectional and masked LMs) its surrounding contexts.
\end{displayquote}
Es así que las autoras los suelen abreviar con LMs (\textit{language models}) por sus siglas en inglés. Actualmente en la literatura se suele usar la abreviación LLMs (\textit{large language models}) haciendo referencia al enorme tamaño de los datasets (datos de entrenamiento) a partir de los cuales se construye el modelo. 
\par Las autoras presentan un conjunto de varias situaciones problemáticas, en resumen:
\begin{enumerate}
    \item Costo ambiental y financiero (\S 3)
    \item Datos de entrenamiento que superan la capacidad humana de entenderlos y documentarlos (\S 4).
    \begin{enumerate}
        \item Aunque se obtiene una gran cantidad de datos, estos no necesariamente son representatitvos de puntos de vista no hegemónicos (e.g. sur global, hablantes de lenguas indígenas, etcétera).
        \item Aún cuando la realidad social cambia, es costoso reentrenar a los modelos con nuevos datos de entrenamiento. Así los LLMs se quedan con una visión estática de nuevos marcos conceptuales\footnote{No encontré una referencia en línea disponible, pero el Grupo de Ingeniería Lingüística de la UNAM trabaja en detección de homofobia usando modelos de lenguaje.}.
        \item Al no curar los datos, los LLMs pueden quedar entrenados para replicar los sesgos existentes en sus datos de entrenamiento.
        \item Al no invertir en la curación de estos datos de entrenamiento se genera una deuda documental con respecto a éstos.
    \end{enumerate}
    \item Las salidas (\textit{outputs}) de los LLMs se pueden malinterpretar como entendimiento del lenguaje natural (NLU).
\end{enumerate}
Es particularmente este último problema el que origina la necesidad de distinguir entre la salida un LLM y texto\footnote{En el contexto de los LLMs multimodales debemos pensar en texto como cualquier representación digital, ya sea texto plano, audio, imágenes, video, código, etcétera.} producido por un ser humano. Las autoras terminan \S 7 con la siguiente pregunta:
\begin{displayquote}
    Could LMs be built in such a way that synthetic text generated with them would be watermarked and thus detectable?
\end{displayquote}
\par La respuesta de la industria ha sido ni siquiera entretener la pregunta. ChatGPT se liberó al mundo sin este tipo de \textit{watermarking} que pudiera hacer su salida detectable. Sin embargo han surgido técnicas que permiten identificar la salida de estos LLMs sin necesidad de la cooperación (directa) de sus desarrolladores. Particularmente, ha habido un desarrollo reciente con respecto al uso de códigos correctores de errores para la detección de éstos.
\subsection{Códigos Correctores de Errores}
\subsubsection{Tokenización}
\label{sec:token}
\par Antes de poder comenzar a hablar acerca del uso de códigos correctores de errores (ECC, por sus siglas en inglés) hay que recordar que nuestro espacio es de la forma $\{0,1\}^k$ para alguna $k\in\zz$, es así que necesitamos una manera mediante la cual podamos convertir lenguaje natural a nuestro vocabulario.
\par Usando Byte-Pair Encoding (BPE) \cite{sennrich_neural_2016} realizamos un análisis del corpus que nos brinda un vocabulario a nivel subpalabra originalmente pensado para traducción automática pero que se presta para el entrenamiento de diferentes arquitecturas. Se puede consultar un ejemplo del tokenizador que usa el LLMs de Meta (llama) en el siguiente \href{https://colab.research.google.com/drive/1GlNBf0DbZTonpNINr_cJQ9VolosbFdR4?usp=sharing}{notebook}.
\par Por último, lo que resulta de este proceso de tokenización es un identificador único que es capaz de lidiar con palabras que nunca antes ha visto o que son compuestas. En el notebook anterior usé \textit{psiconeuroinmunoendocrinología} y un saludo del náhuatl, y pese a ser cadenas de texto de baja (o nula) frecuencia en los datos de entrenamiento, el modelo es capaz de asociarlos a componentes a nivel subpalabra.
\subsubsection{Definiciones}
\label{sec:def}
Chao et al. \cite{chao_watermarking_2025} toman a $k\in\zz^+$, la distancia de Hamming $d_H$ en $\{0,1\}^k$ con $d_H(u,v)$ la cantidad de coordenadas que difieren entre $u$ y $v$ \cite{kerl_introduction_2004} para así definir 
\begin{definition}
    \label{def:ecc}
    Sean $k,n\in\zz^+$ con $k\leqslant n$, un \textit{código corrector de errores (ECC)} es un mapeo inyectivo $\mathcal{C}:\{0,1\}^k\hookrightarrow\{0,1\}^n$.
\end{definition}
\par De aquí se puede definir que la \textit{codificación} de un mensaje $m$ se obtiene aplicándole $\mathcal{C}$ y $\mathcal{C}(m)$ se define como la \textit{codeword} o palabra clave. Por último la tasa de codificación es $k/n$.
\par De igual manera, podemos definir la distancia de corrección de error de $\mathcal{C}$. Ésta es la $t>0$ más grande tal que para cualquier mensaje arbitrario $m\in\{0,1\}^n$ existe a lo más una palabra clave $c\in\mathcal{C}$ en a bola de centro $m$ y radio $t$ con la distancia de Hamming. En otras palabras buscamos a la $t$ más grande tal que existe una única $c$ que cumple que $d_H(m,c)\leqslant t$. Podemos caracterizar a nuestro código con esta $t$ y podemos decir que $\mathcal{C}$ es un $[n, k, 2t+1]$-código.
\par Dado un $[n, k, 2t+1]$-código podemos pensar en un mapeo decodificador $\mathcal C^{-1}$. Veamos que está bien definido pues por construcción para cualquier $m\in\{0,1\}^n$ existe a lo más un $c=\mathcal{C}^{-1}(m)$ tal que $d_H(w,c)\leqslant t$. En caso de no existir, la decodificación se puede escoger de manera arbitraria.
\section{Watermarking}
\subsection{Técnicas de watermarking}
%\subsubsection{Correlated Binary Sampling Channel (CBSC)}
\subsubsection{Hard Red List}
\subsubsection{Soft Red List}
\subsubsection{Robust Binary Code (RBC)}
\par Conectando el \cref{sec:def} y el \cref{sec:token} podemos pensar en el conjunto de datos de entrenamiento que tenemos para un LLM. Posteriormente, podemos descomponerlo a nivel subpalabra, permitiéndonos procesar vocabulario que no se encuentra en nuestro corpus. De cualquier forma, estas subpalabras se asocian a un número que tiene una representación binaria en forma de bits. Es así que para un vocabulario\footnote{de subpalabras, aunque no es forzoso verlo de esta manera. Existen algoritmos diferentes al BPE que son capaces de procesar palabras íntegras como Bag of Words (BoW).} $\mathcal{V}$ de tamaño $|\mathcal V|$ necesitamos $l=\lceil\log_2|\mathcal V|\rceil$. De aquí podemos usar nuestro algoritmo que convierte a binario $\Gamma:\mathcal V\to\{0,1\}^l$.
\par Ahora podemos pensar en la distribución\footnote{Aunque es una interpretación común pensar en esta distribución, a nivel computacional es muy costoso obtener esta probabilidad. Los atajos computacionales usados como el \textit{top-k sampling} devuelven un número muy cercano al que debería ser la probabilidad real pero formalmente esta distribución no está bien definida. Sin embargo, en su uso cotidiano, no se suele hacer esta distinción.} inducida dado un token cualquiera, como los LLMs se entrenan para servir en tareas de predicción de texto tiene sentido preguntarse
\begin{displayquote}
    Dados los primeros $n$ bits, ¿cuál es la probabilidad de que el siguiente bit sea un uno?
\end{displayquote}
Denotemos a los primeros $n$ bits como $b_{1:n}$ podemos denotar esta probabilidad con $q_{n+1}$ tal que
\[q_{n+1}=\pp(B_{n+1}=1|b_{1:n})\]
Ahora tomemos a nuestro mensaje $m=\{M_i\}_{i=1}^{|m|}\subset\{0,1\}^k$ con $|m|$ la cantidad de tokens en el mensaje. Podemos escoger a un ECC $\mathcal C$ al del cual podemos obtener a una palabra clave $Y_i=\mathcal C(M_i)\in\{0,1,\}^n$. Posteriormente podemos tomar una muestra de bits $(B_i)_{i=1}^n$ correlacionada con $Y_i$.
\par Para obtener esta muestra definimos una v.a. $B\sim\textrm{Ber}(q)$ con $q\in[0,1]$, por construcción podemos simular una Bernoulli a partir de una v.a. $U'\sim \textrm{Unif}[0,1]$ tal que
\[B=\uno[\{U'\leqslant q\}]\]
Una consecuencia inmediata es que $U',B$ son v.a. correlacionadas. Ahora sea veamos que para una $\textrm{Ber}(1,2)$
\[f(k;\frac{1}{2})=\left(\frac{1}{2}\right)^k\left(\frac{1}{2}\right)^{1-k}=\frac{1}{2}\]
Es así que podemos observar que $B=\uno[\Big\{\big((1-Y)+U\big)/2\leqslant q\Big\}]\sim \textrm{Ber}(q)$ para $Y\sim\textrm{Ber}\left(\frac{1}{2}\right)$ y $U\sim\textrm{Unif}[0,1]$ vemos que por el parámetro de $Y$ podemos pensar en que $(1-Y)$ nos permite obtener un bit $B\sim\textrm{Ber}(q)$ de una manera no uniforme, sesgada a $Y$. A este proceso se le llama \textit{Correlated Binary Sampling Channel} (CBSC).
\par Una de las propiedades más importantes que tiene es que $\pp[Y=B]=1-2|1/2-q|\geqslant\frac{1}{2}$. En particular la desigualdad se vuelve estricta cuando $q\not\in\{0,1\}$ (que podemos pensar como una medida de la entropía de $B$ y por tanto cuando $q=0,1$ siempre sabremos cuánto valdrá $B$), por lo que $B$ está sesgada a $Y$. 
\par Esta técnica aprovecha este sesgo pero que define a $Y=(Y_1,\cdots,Y_n)$ a partir de una serie de bits conocida. En general por el desarrollo anterior, podemos ver que los bits generados serán sesgados a $Y$ permitiéndonos identificar que provienen de un modelo de lenguaje con la marca de agua explicada anteriormente. 
\par Además RBC utiliza ECCs para modificar en medida de lo posible a $d_H(B,Y)$ haciendo que en general $d_H(B,Y)<\frac{n}{2}$. Esto mediante la selección de un mensaje $M\in\{0,1\}^k$ que por la \cref{def:ecc} hace que cuando tomemos un $[n,k,2t+1]$-ECC $\mathcal C$ seamos capaces de tener la capacidad de corregir hasta $t$ errores en $Y=\mathcal C(M)\in\{0,1\}^n$.
\paragraph{Robust Binary Code.} Ahora, retomando todo lo detallado hasta este momento podemos detallar el algoritmo de Chao et Al\cite{chao_watermarking_2025}. 
\par Teniendo $\mathcal V$ el vocabulario de tamaño $|\mathcal V|$ obtenido de nuestra corpora, definimos a $l=\lceil\log_2|\mathcal V|\rceil$ la cantidad de bits necesarios para almacenar cada token en binario, entonces tenemos a $\Gamma:\mathcal V\to\{0,1\}^l$ nuestro convertidor binario con su inversa $\Gamma^{-1}$\footnote{Donde podemos solucionar el problema de la inexistencia de la imagen inversa tomando una $l$ suficientemente grande de modo que podamos tomar arbitrariamente un elemento de $\{0,1\}^l$ para cada elemento de $\mathcal V$}. Tomamos también a un $[n,k,2t+1]$-ECC $\mathcal C$ y definimos a la ventana de entrada $w_{\textrm{in}}=\left\lceil\frac{k}{l}\right\rceil$, la ventana de salida $w_{\textrm{out}}=\left\lceil\frac{k}{l}\right\rceil$, y la variable aleatoria de cadenas de texto binario $R\sim\textrm{Unif}\{0,1\}^k$. De igual forma podemos tomar a nuestro modelo de lenguaje y tomar la distribución del $n+1$-ésimo token dada una ventana de $n$ tokens en su expresión binaria como
\[p_{\textrm{bin}}(s_1,...,s_n)=p_{\textrm{bin}}(S_{n+1}|s_1,...,s_n)\]
Por último, dados una cantidad de $w_{\textrm{in}}$ tokens $X_i$ generados por el modelo de lenguaje y $N$ la cantidad de tokens a generar:
\begin{enumerate}
    \item Iteramos desde $w_{\textrm{in}}+1$ hasta $N$. En cada iteración $i$ realizamos lo siguiente:
    \begin{enumerate}
        \item Definimos el mensaje $M\in\{0,1\}^k$ como $M=\Gamma(X_{i-w_{\textrm{in}}:(i-1)})_{1:k}  \oplus R$
        \item Definimos a $Y=\mathcal{C}(M)\in\{0,1\}^n$
        \item Creamos $w_{\textrm{out}}$ tokens de la siguiente manera
        \begin{enumerate}
            \item Tomamos una muestra 
            desde $j=1$ hasta $n$ bits $B_j$ 
            usando el muestreo sesgado a $Y$
            descrito con anterioridad con $q=p_{\textrm{bin}}(X,B_{1:(j-1)})$
            %$q=p_{\textrm_{bin}}(X,B_{1:(j-1)})$.
            \item Posteriormente, tomamos una muestra desde el bit $j=n+1$ hasta el bit $w_{\textrm{out}}\cdot l$ con una Bernoulli de parámetro $q=p_{\textrm{bin}}(X,B_{1:(j-1)})$ con $X$ el texto generado con anterioridad
            \item Por último regresamos el volcabulario correspondiente a $B$, es decir $\Gamma^{-1}(B)$
        \end{enumerate}
        \item Aumentamos a $i$ por $w_{\textrm{out}}$ unidades
    \end{enumerate}
    \item Al terminar las iteraciones, regresamos todos los tokens generados
\end{enumerate}
Con $\oplus$ el operador bit a bit XOR.

\subsection{Detección de watermarking}
Para llevar a cabo la detección de nuestra marca de agua tomamos la distancia de Hamming del mensaje $M_i$, es decir el obtenido en cada iteración del RBC, y lo comparamos con el mensaje que se obtiene aplicando el decodificador $\mathcal C^{-1}$ del bloque que se generó usando $M_i$. Tras cada cálculo, se le resta a $k$ y se realiza por cada una de las ventanas ($N-w_{\textrm{out}}-w_{\textrm{in}}+1$). Al final se realiza una prueba de comparación binomial, donde si se detecta que los tokens se generaron con un sesgo a nuestra distribución binomial, entonces se detectará la marca de agua.
\section{Validación experimental}
Se puede consultar el consultar el \href{https://colab.research.google.com/drive/1GlNBf0DbZTonpNINr_cJQ9VolosbFdR4?usp=sharing}{notebook} dado con anterioridad para ver un ejemplo de cómo se implementan los algoritmos anteriores.
\section{Conclusión}
Aunque existían métodos de watermarking previos a la escritura de la referencia principal, los Códigos Correctores de Errores permiten sesgar de una manera eficiente y poco detectable a los textos generados por modelos de lenguaje. Esto a su vez nos permite lidiar con algunos de los problemas que uso descontrolado genera a la sociedad, potencialmente permitiendo identificar rápidamente desinformación y propaganda generada de manera automatizada.
\nocite{*}
\printbibliography
\end{document}

Al inicio del proyecto se llevó a cabo un proceso de familiarización con la base de código. Consistió sobre todo en una lectura de la documentación existente, y reuniones acerca de las limitaciones actuales y áreas de oportunidad del mismo. Sobre todo se discutió acerca de su adaptación para ejecutarse desde el clúster de supercómputo Atócatl. 
Una de las grades limitantes al momento de buscar ejecutar el código, era la reproducibilidad del mismo. Se hizo un esfuerzo importante dirigido en dos componentes: hacer explícitos todos los prerrequisitos necesarios para la ejecución del código e identificar el flujo de los diversos programas necesarios y la relación entre ellos. 
Así se realizó un entorno de \texttt{conda} con todas las librerías necesarias y su respectivo YAML para permitir que cualquier usuario con con \texttt{conda}, \texttt{miniconda}, o \texttt{anaconda} pueda importar su entorno y reproducir el código. Con respecto al segundo punto se realizó un script de bash que tiene dos propósitos. El principal es permitir que un solo programa pueda realizar todas las acciones discretas necesarias para la ejecución final de Spelfig para cada galaxia y el secundario es servir como una documentación que señale la estructura de directorios necesaria para que cada programa pueda identificar la existencia de los archivos de entrada.
Actualmente se está llevando a cabo una fase de corridas de prueba con una última modificación al código que permite al usuario tener un archivo con los identificadores de las galaxias que desea ejecutar y las manda a ejecutar, permitiendo así tener un control más granular con respecto a qué datos analizir y permite realizar cambios rápidamente sin necesidad de afectar el código fuente.